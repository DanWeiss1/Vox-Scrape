A computer just passed the Turing test — but no, robots aren't about to take over
Yes, a computer may have just passed the Turing test for artificial intelligence. No, this isn't as big a deal as many news outlets are making it out to be.
This is a gradual improvement on a program from 2012
On Saturday, at an artificial intelligence competition in London, a chatbot named Eugene Goostman successfully convinced 33 percent of a panel of judges that it was actually a 13-year-old boy.
In doing so, the computer seems to have passed the "Turing test" — a theoretical standard for artificial intelligence first proposed in 1950. The idea is that if a computer program can convince more than 30 percent of a group of humans that it too is human after five minutes of chatting, it has achieved some aspect of true artificial intelligence.
At the same time, an earlier version of the same program reached a 29 percent success rate at a competition in 2012, so it's not as though this is a breakthrough achievement — just a gradual improvement. Moreover, the 30 percent figure itself is somewhat arbitrary: Alan Turing, the British mathematician who proposed the test, originally mentioned it as an idle prediction about machine intelligence that would exist in the year 2000, rather than some sort of unequivocal gold standard for AI.
Still, programmers around the world have been creating bots to pass the test for years — and if the results hold up, this would be the very first success. (The results have yet to be independently reviewed.) The chatbot might not be a truly groundbreaking technology, but it is an important milestone on the pathway to artificial intelligence.

A statue of Alan Turing. Wikimedia commons/Jon Callas
Alan Turing — widely considered the father of artificial intelligence — proposed the test in a 1950 paper called "Computing Machinery and Intelligence." In arguing that computers would one day be able to think, he proposed that "thinking" could be interpreted to mean "imitating humans."
Specifically, he proposed a setup in which a human judge chats with both another human and a computer program via a text chat. If the program is sophisticated enough to imitate human behavior to the degree that the judge can't reliably tell the human and computer apart, Turing argued, it is "thinking" in some meaningful way. (Text was used so the computer wouldn't need to imitate the inflections of a human voice.)
in arguing that computers would one day be able to think, turing proposed that "thinking" be interpreted to mean "imitating humans"
The 30 percent standard and five-minute chatting period came about in an aside: Turing was disclosing his personal beliefs about AI, and simply mentioned that he expected a program to be able to cross this threshold by the year 2000.
Soon afterward, programmers began developing primitive programs they hoped would be able to pass that 30 percent test. In 1991, the Loebner Prize — an annual competition among computer programs (and programmers) to pass the Turing test — was established.
The chatbots that participate in this and other competitions have slowly gotten more sophisticated over the years. The early ones simply scanned a human's text input for certain keywords and answered with responses vaguely based on them (if you ever chatted with AIM's SmarterChild, these bots might be familiar to you).
More recent ones are built from the same basic template, but they're more sophisticated, and programmed to demonstrate very human tendencies — like sarcasm, irreverence, and playfulness — in order to fool human judges. Over the past few years, various programs have come close to crossing the 30 percent barrier — in many cases, if they'd deceived a human one more time, they would have passed the Turing test. 

A screenshot of Eugene Goostman, the chatting program that may have passed the test.
On Saturday, at an event organized by the University of Reading in the UK, a program named Eugene Goostman — developed by the computer scientists Vladimir Veselov and Eugene Demchenko — convinced a third of a panel of judges that it too was human, leading the event organizers to claim it had passed the Turing test.
The program, first created in 2001 and refined over the years, had previously finished in second place at several Loebner Prize competitions and hit a 29 percent success rate. It's designed to resemble a 13-year-old boy from Odessa, Ukraine.
the program is designed to resemble a 13-year-old boy from odessa, ukraine
"Our main idea was that he can claim that he knows anything, but his age also makes it perfectly reasonable that he doesn't know everything," Veselov said in a press release. "We spent a lot of time developing a character with a believable personality." 
The character has a pet guinea pig, likes hamburgers and candy, and has a father who is a gynecologist. A version of Eugene is hosted online so anyone can chat with it, but it appears to be down at the moment.
The event organizers claim that the result was independently verified and that their competition involved more judges than any previous event, which would make it less likely to be the result of randomness, or a few judges particularly bad at telling humans from computers.
Still, the outcome hasn't been formally peer-reviewed, so it's not conclusive just yet.
This announcement certainly doesn't mean that self-aware robots are about to take over the world — and it doesn't even mean that there's one out there capable of consistently fooling people into thinking its a human.
It does, however, mean that one has crossed the threshold Turing predicted would be passed by 2000, a meaningful milestone on the way to artificial intelligence.
That said, there are plenty more milestones that still need to be passed — even in terms of the Turing test. The Loebner prize, for instance, will award a silver medal for the first program to pass a text-only test, but a gold medal for one that passes an audio test — something that's probably still a long way off.
this announcement doesn't mean that self-aware robots are about to take over the world
Moreover, there are many computer scientists and philosophers out there who criticize the idea of the Turing test in the first place — and argue that merely being able to trick humans doesn't constitute true artificial intelligence.
For one, the test just proves that a computer can imitate humans — who, in many cases, act unintelligently. Making typos and grammatical mistakes, for instance, might enhance a program's success, even though they really have nothing to do with artificial intelligence.
One of the most famous objections, by philosopher John Searle, posits that even if a computer can reliably interpret symbols (in this case, written English) to the degree that it can provide sensical responses, it can't be said to be truly "conscious" because it doesn't really understand what the symbols mean.
Still, if Eugene Goostman's success is ultimately verified, it'd be an exciting moment for the burgeoning field of artificial intelligence — and one technology historians might someday look back at with wonder.
Further reading:
